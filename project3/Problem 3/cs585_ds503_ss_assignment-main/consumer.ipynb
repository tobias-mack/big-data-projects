{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccc4d53f-2b13-43a1-a554-82b06b0cf764",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeaedfdf-fc0d-4ff0-b21d-a9097fb241fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"global\": {\n",
      "    \"kafka_bootstrap_servers\": \"kafka:9092\",\n",
      "    \"kafka_topic\": \"test-structured-streaming\",\n",
      "    \"kafka_consumer_group\": \"ss_job\",\n",
      "    \"max_records_per_batch\": 20\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import json\n",
    "import logging\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"consumer\")\n",
    "\n",
    "\n",
    "config = json.load(open(\"config.json\"))\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92965dc3-a7c7-40ea-bfdb-1e02340b3713",
   "metadata": {},
   "source": [
    "# 2. Initialize Spark with Kafak Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed753070-30f8-4883-97aa-4147a6532147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.kafka#kafka-clients added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.google.guava#guava added as a dependency\n",
      "org.apache.httpcomponents#httpcore added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-59127622-ba15-4f11-bf8f-005c26599de5;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.32 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.0 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.563 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound com.google.guava#guava;21.0 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.8 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.3.0/spark-sql-kafka-0-10_2.12-3.3.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0!spark-sql-kafka-0-10_2.12.jar (172ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.0/hadoop-aws-3.3.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.3.0!hadoop-aws.jar (207ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/guava/guava/21.0/guava-21.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.guava#guava;21.0!guava.jar(bundle) (460ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcore/4.4.8/httpcore-4.4.8.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.httpcomponents#httpcore;4.4.8!httpcore.jar (106ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.3.0/spark-token-provider-kafka-0-10_2.12-3.3.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0!spark-token-provider-kafka-0-10_2.12.jar (72ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.8.1/kafka-clients-2.8.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;2.8.1!kafka-clients.jar (654ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (51ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (78ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (68ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.2/hadoop-client-runtime-3.3.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.2!hadoop-client-runtime.jar (4110ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (147ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.8.4/snappy-java-1.1.8.4.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.8.4!snappy-java.jar(bundle) (333ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.32/slf4j-api-1.7.32.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.32!slf4j-api.jar (67ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.2/hadoop-client-api-3.3.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.2!hadoop-client-api.jar (2828ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...\n",
      "\t[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (60ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.563/aws-java-sdk-bundle-1.11.563.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.11.563!aws-java-sdk-bundle.jar (21806ms)\n",
      "downloading https://repo1.maven.org/maven2/org/wildfly/openssl/wildfly-openssl/1.0.7.Final/wildfly-openssl-1.0.7.Final.jar ...\n",
      "\t[SUCCESSFUL ] org.wildfly.openssl#wildfly-openssl;1.0.7.Final!wildfly-openssl.jar (124ms)\n",
      ":: resolution report :: resolve 11113ms :: artifacts dl 31375ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.563 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcom.google.guava#guava;21.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.8 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.32 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 by [org.apache.kafka#kafka-clients;2.8.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   17  |   17  |   1   ||   17  |   17  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-59127622-ba15-4f11-bf8f-005c26599de5\n",
      "\tconfs: [default]\n",
      "\t17 artifacts copied, 0 already retrieved (187461kB/387ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/15 23:18:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "INFO:consumer:Spark Driver memory: None\n",
      "INFO:consumer:Spark Executor memory: None\n",
      "INFO:consumer:Loaded jars:\n",
      "[\n",
      "  \"spark://2537ba3b59b5:37211/jars/com.amazonaws_aws-java-sdk-bundle-1.11.563.jar\",\n",
      "  \"spark://2537ba3b59b5:37211/jars/org.spark-project.spark_unused-1.0.0.jar\",\n",
      "  \"spark://2537ba3b59b5:37211/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar\",\n",
      "  \"spark://2537ba3b59b5:37211/jars/org.apache.commons_commons-pool2-2.11.1.jar\",\n",
      "  \"spark://2537ba3b59b5:37211/jars/com.google.guava_guava-21.0.jar\",\n",
      "  \"spark://2537ba3b59b5:37211/jars/com.google.code.findbugs_jsr305-3.0.0.jar\",\n",
      "  \"spark://2537ba3b59b5:37211/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar\",\n",
      "  \"spark://2537ba3b59b5:37211/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar\",\n",
      "  \"spark://2537ba3b59b5:37211/jars/commons-logging_commons-logging-1.1.3.jar\",\n",
      "  \"spark://2537ba3b59b5:37211/jars/org.lz4_lz4-java-1.8.0.jar\",\n",
      "  \"spark://2537ba3b59b5:37211/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar\",\n",
      "  \"spark://2537ba3b59b5:37211/jars/org.slf4j_slf4j-api-1.7.32.jar\",\n",
      "  \"spark://2537ba3b59b5:37211/jars/org.apache.kafka_kafka-clients-2.8.1.jar\",\n",
      "  \"spark://2537ba3b59b5:37211/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar\",\n",
      "  \"spark://2537ba3b59b5:37211/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar\",\n",
      "  \"spark://2537ba3b59b5:37211/jars/org.apache.hadoop_hadoop-aws-3.3.0.jar\",\n",
      "  \"spark://2537ba3b59b5:37211/jars/org.apache.httpcomponents_httpcore-4.4.8.jar\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Create Spark Configuration and set application name\n",
    "conf = SparkConf().setAppName(\"KafkaExp\")\n",
    "\n",
    "# Default pyspark installation lacks kafka consumer libraries. Install kafka-client libs manually\n",
    "kafka_packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{\"2.12\"}:{\"3.3.0\"}',\n",
    "    'org.apache.kafka:kafka-clients:2.8.0',\n",
    "    \"org.apache.hadoop:hadoop-aws:3.3.0\",\n",
    "    \"com.google.guava:guava:21.0\",\n",
    "    \"org.apache.httpcomponents:httpcore:4.4.8\"\n",
    "]\n",
    "\n",
    "# Provide kafka jar paths to driver and executors\n",
    "kafka_jar_paths = '/mnt/home/prathyush/.ivy2/jars/'.join([\n",
    "    \"org.apache.hadoop_hadoop-client-runtime-3.3.2.jar\",\n",
    "    \"org.apache.kafka_kafka-clients-2.8.1.jar\",\n",
    "    \"hadoop-aws-2.7.5.jar\",\n",
    "    \"aws-java-sdk-core-1.12.268.jar\"\n",
    "])\n",
    "\n",
    "# Connect to Spark cluster (Cluster mode instead of local mode)\n",
    "conf = (conf.setMaster('spark://spark:7077')\n",
    "        .set('spark.jars.packages', ','.join(kafka_packages))\n",
    "        .set('spark.driver.extraClassPath', '/mnt/home/prathyush/.ivy2/jars/*')\n",
    "        .set('spark.executor.extraClassPath', '/mnt/home/prathyush/.ivy2/jars/*')\n",
    "        )\n",
    "\n",
    "# Create spark context\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "logger.info(f\"Spark Driver memory: {sc._conf.get('spark.driver.memory')}\")\n",
    "logger.info(f\"Spark Executor memory: {sc._conf.get('spark.executor.memory')}\")\n",
    "logger.info(\n",
    "    f'Loaded jars:\\n{json.dumps((sc._jsc.sc().listJars().toList().toString().replace(\"List(\", \"\").replace(\")\", \"\").split(\", \")), indent=2)}')\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Create spark session\n",
    "spark = SparkSession(sc)\n",
    "spark.conf.set(\"spark.sql.parquet.compression.codec\", \"gzip\")\n",
    "spark.conf.set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\")\n",
    "spark.conf.set(\"parquet.enable.summary-metadata\", \"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680b4715-66b6-4a47-8511-fd57bf5a0b52",
   "metadata": {},
   "source": [
    "# 3. Test Kafka topic and connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd64ee87-38ef-48fc-9c3a-cb02eea054b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kafka Connection successful!\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka.admin import AdminClient\n",
    "\n",
    "def test_kafka_connection(broker_conf:dict) -> None:\n",
    "    \"\"\"\n",
    "    Function to test kafka connection\n",
    "    :param broker_conf: Broker configuration\n",
    "    :returns: None\n",
    "    \"\"\"\n",
    "    client = AdminClient(broker_conf)\n",
    "    topics = client.list_topics().topics\n",
    "    if not topics:\n",
    "        raise RuntimeError()\n",
    "    print(\"Kafka Connection successful!\")\n",
    "\n",
    "\n",
    "broker_conf = {\n",
    "    'bootstrap.servers': config[\"global\"][\"kafka_bootstrap_servers\"]\n",
    "}\n",
    "\n",
    "# Test kafka connection\n",
    "test_kafka_connection(broker_conf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56812b03-56c0-45d2-8e3f-8e96fb354d86",
   "metadata": {},
   "source": [
    "# 4. Load Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e7ef60c-be21-4f7c-be0d-d02115d52d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load schema \n",
    "schema = pickle.load(open(\"schema.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e3505e-9cef-4148-a0af-10643d9d94c7",
   "metadata": {},
   "source": [
    "# 5. Configure Spark-Kafka consumer options and Subscribe to Kafka Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f9c1b40-46a6-4629-834e-5df667667891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Configure spark kafka client options\n",
    "spark_kafka_options = {\n",
    "    \"kafka.bootstrap.servers\": config[\"global\"][\"kafka_bootstrap_servers\"],\n",
    "    \"subscribe\": config[\"global\"][\"kafka_topic\"],\n",
    "    \"kafka.group.id\": config[\"global\"][\"kafka_consumer_group\"],\n",
    "    \"maxOffsetsPerTrigger\": config[\"global\"][\"max_records_per_batch\"],\n",
    "    \"startingOffsets\": \"earliest\",\n",
    "}\n",
    "\n",
    "# Enable spark read stream\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", spark_kafka_options[\"kafka.bootstrap.servers\"]) \\\n",
    "  .option(\"subscribe\", spark_kafka_options[\"subscribe\"]) \\\n",
    "  .option(\"kafka.group.id\", spark_kafka_options[\"kafka.group.id\"]) \\\n",
    "  .option(\"maxOffsetsPerTrigger\", spark_kafka_options[\"maxOffsetsPerTrigger\"]) \\\n",
    "  .option(\"startingOffsets\", spark_kafka_options[\"startingOffsets\"]) \\\n",
    "  .load()\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "#df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\").as[(String, String)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fced3126-ff85-4d64-82bd-655c935e9711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Json File Handler Class\n",
    "class JsonFileHandler(object):\n",
    "    def __init__(self, file_path:str, mode:str):\n",
    "        \"\"\"\n",
    "        Initialize file handler\n",
    "        \"\"\"\n",
    "        self.f = open(file_path, mode)\n",
    "    \n",
    "    def write_dataframe_as_jsonl(self, batch_df):\n",
    "        \"\"\"\n",
    "        Write a micro-batch dataframe to a jsonl file \n",
    "        \"\"\"\n",
    "        for row in batch_df.collect():\n",
    "            res = json.dumps(row.asDict())\n",
    "            self.f.write(res+\"\\n\")\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Finalize file write object\n",
    "        \"\"\"\n",
    "        self.f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6acb91e0-f8f4-4907-8fbd-13d2ee5f44fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a JSON File Writer\n",
    "json_file_writer = JsonFileHandler(file_path=\"result.jsonl\", mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7f05808-04b3-4002-a903-23eb92255ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max(years):\n",
    "    if not years:\n",
    "        return []\n",
    "    return [min(years),max(years)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b022a19-12ed-46ab-800f-97503e2a6917",
   "metadata": {},
   "source": [
    "# 6. Start spark structred streaming job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4934c6a5-164b-42e6-9e51-963b420490e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Python Server ready to receive messages\n",
      "INFO:py4j.clientserver:Received command c on object id p2\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 | 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lambda Function for processing each batch of record\n",
    "def process_batch(batch_df, batch_idx):\n",
    "    print(f\"{batch_idx} | {batch_df.count()}\")\n",
    "\n",
    "    # Process data and calculate\n",
    "    # a. age\n",
    "    # b. num_contribs\n",
    "    # c. min_max_years\n",
    "\n",
    "    batch_df = batch_df.selectExpr(\"CAST(value AS STRING)\").select(F.from_json(\"value\", schema).alias(\"data\"))\n",
    "    batch_df = batch_df.selectExpr(\"data.name\",\"data.contribs\",\"data.awards\",\"CAST(data.birth AS DATE) as birth\",\"CAST(data.death AS DATE) as death\")\n",
    "    batch_df = batch_df.withColumn('death', F.when(F.col('death').isNull(), datetime.datetime.now().date()).otherwise(F.col('death')))\n",
    "    batch_df= batch_df.withColumn(\"age\", F.year(F.col(\"death\"))-F.year(F.col(\"birth\")))\n",
    "    convertUDF = F.udf(lambda z: min_max(z))\n",
    "    \n",
    "    # Select required columns -  \"name\", \"age\", \"num_contribs\", \"min_max\"\n",
    "    batch_df = batch_df.select(F.col(\"name\"), F.col(\"age\"),F.size(F.col(\"contribs\")).alias(\"num_contribs\"), convertUDF(F.col(\"awards.year\")).alias(\"min_max\"))\n",
    "\n",
    "    # Save to parquet file - result.parquet \n",
    "    json_file_writer.write_dataframe_as_jsonl(batch_df)\n",
    "    return batch_df\n",
    "\n",
    "# Structred streaming query\n",
    "query = df.writeStream.foreachBatch(process_batch).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08164cc-18ff-4e14-82b2-6b69af325de0",
   "metadata": {},
   "source": [
    "# 7. Monitor structred streaming job progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c34f9d7b-bd97-42db-b5c9-74373f999bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:consumer:Structred streaming job completed successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'Waiting for data to arrive', 'isDataAvailable': False, 'isTriggerActive': False}\n"
     ]
    }
   ],
   "source": [
    "# Add startup delay\n",
    "time.sleep(5)\n",
    "# Update Job Status\n",
    "\n",
    "print(query.status)\n",
    "while query.status['isDataAvailable'] or query.status['isTriggerActive']:\n",
    "    print(query.status)\n",
    "    time.sleep(5)\n",
    "\n",
    "# Stop query\n",
    "query.stop()\n",
    "json_file_writer.close()\n",
    "\n",
    "logger.info(\"Structred streaming job completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b537073b-dbd2-4742-9154-e9b7056702f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------------------+------------+\n",
      "|age|     min_max|                name|num_contribs|\n",
      "+---+------------+--------------------+------------+\n",
      "| 83|[1967, 1993]|[null, John, Backus]|           4|\n",
      "| 84|[1971, 1990]|[null, John, McCa...|           3|\n",
      "| 86|[1969, 1991]|[null, Grace, Hop...|           4|\n",
      "| 76|[1999, 2001]|[null, Kristen, N...|           2|\n",
      "| 71|[1999, 2001]|[null, Ole-Johan,...|           2|\n",
      "| 66|[2001, 2003]|[null, Guido, van...|           1|\n",
      "| 70|[1983, 2011]|[null, Dennis, Ri...|           2|\n",
      "| 57|[2011, 2011]|[Matz, Yukihiro, ...|           1|\n",
      "| 67|[2002, 2007]|[null, James, Gos...|           1|\n",
      "| 67|          []|[null, Martin, Od...|           1|\n",
      "+---+------------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"result.jsonl\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ad6102-ebc8-429d-baac-6fb9d4adf8e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "5eab687acb3ddfe264791fe74937bc8765d50ea3df4d9a9a62730aa97325aae8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
